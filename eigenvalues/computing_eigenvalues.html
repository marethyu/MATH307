

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Computing Eigenvalues &#8212; Applied Linear Algebra</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/main.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/main.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-VXJGYXTXDR"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-VXJGYXTXDR');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'eigenvalues/computing_eigenvalues';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Complex Vectors" href="../dft/complex.html" />
    <link rel="prev" title="Singular Value Decomposition" href="svd.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/nn.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/nn.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Systems of Equations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../systems/lu.html">LU Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../systems/error.html">Error Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../systems/interpolation.html">Interpolation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../systems/odes.html">Differential Equations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Orthogonality</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../orthogonality/subspaces.html">Subspaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../orthogonality/complement.html">Orthogonal Complement</a></li>
<li class="toctree-l1"><a class="reference internal" href="../orthogonality/projection.html">Orthogonal Projection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../orthogonality/qr.html">QR Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../orthogonality/least_squares.html">Least Squares Approximation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Eigenvalues</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="diagonalization.html">Diagonalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="svd.html">Singular Value Decomposition</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Computing Eigenvalues</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Dicrete Fourier Transform</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../dft/complex.html">Complex Vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dft/dft.html">Discrete Fourier Transform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dft/frequency.html">Frequency, Amplitude and Phase</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dft/fft.html">Fast Fourier Transform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dft/convolution.html">Convolution and Filtering</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notebooks/01_linear_systems.html">Linear Systems of Equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/02_LU_decomposition.html">LU Decomposition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/03_polynomial_interpolation.html">Polynomial Interpolation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/04_spline_interpolation.html">Natural Cubic Spline Interpolation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/05_finite_difference_method.html">Finite Difference Method</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/06_least_squares_regression.html">Fitting Models to Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/07_pca.html">Principal Component Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/08_deblurring_images.html">Deblurring Images</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/09_computed_tomography.html">Computed Tomography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/10_computing_eigenvalues.html">Computing Eigenvalues</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/12_fft.html">Discrete Fourier Transform</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../appendix/multiplication.html">Matrix Multiplication</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/UBCMath/math307" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/eigenvalues/computing_eigenvalues.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Computing Eigenvalues</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#power-method">Power Method</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rayleigh-quotient">Rayleigh Quotient</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inverse-iteration">Inverse Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pagerank">PageRank</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="computing-eigenvalues">
<h1>Computing Eigenvalues<a class="headerlink" href="#computing-eigenvalues" title="Permalink to this heading">#</a></h1>
<div class="bigidea docutils">
<p>It is not practical to compute eigenvalues of a matrix <span class="math notranslate nohighlight">\(A\)</span> by finding roots of the characteristic polynomial <span class="math notranslate nohighlight">\(c_A(x)\)</span>. Instead, there are several efficient algorithms for numerically approximating eigenvalues without using <span class="math notranslate nohighlight">\(c_A(x)\)</span> such as the power method.</p>
</div>
<section id="power-method">
<h2>Power Method<a class="headerlink" href="#power-method" title="Permalink to this heading">#</a></h2>
<div class="definition docutils">
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be a square matrix. An eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span> of <span class="math notranslate nohighlight">\(A\)</span> is called a <strong>dominant eigenvalue</strong> if <span class="math notranslate nohighlight">\(\lambda\)</span> has (algebraic) multiplicity 1 and <span class="math notranslate nohighlight">\(| \lambda | &gt; | \mu |\)</span> for all other eigenvalues <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
</div>
<div class="definition docutils">
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix with eigenvalues <span class="math notranslate nohighlight">\(\lambda_1,\dots,\lambda_n\)</span> and corresponding eigenvectors <span class="math notranslate nohighlight">\(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n\)</span> with dominant eigenvalue <span class="math notranslate nohighlight">\(\lambda_1\)</span>. Let <span class="math notranslate nohighlight">\(\boldsymbol{x}_0\)</span> be any vector which is a linear combination of the eigenvectors of <span class="math notranslate nohighlight">\(A\)</span></p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{x}_0 = c_1 \boldsymbol{v}_1 + \cdots + c_n \boldsymbol{v}_n
\]</div>
<p>such that <span class="math notranslate nohighlight">\(c_1 \not= 0\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
A^k \boldsymbol{x}_0 = c_1 \lambda_1^k \boldsymbol{v}_1 + \cdots + c_n \lambda_n^k \boldsymbol{v}_n
\]</div>
<p>and therefore</p>
<div class="math notranslate nohighlight">
\[
(1/\lambda_1^k)A^k \boldsymbol{x}_0 = c_1 \boldsymbol{v}_1 + c_2 (\lambda_2/\lambda_1)^k \boldsymbol{v}_2 + \cdots + c_n (\lambda_n/\lambda_1)^k \boldsymbol{v}_n
\ \ \rightarrow \ \ c_1 \boldsymbol{v}_1 \ \ \text{as } k \to \infty
\]</div>
<p>because each term <span class="math notranslate nohighlight">\(| \lambda_i/\lambda_1 | &lt; 1\)</span> and so <span class="math notranslate nohighlight">\(\lambda_i/\lambda_1 \to 0\)</span> as <span class="math notranslate nohighlight">\(k \to \infty\)</span>. This method of approximating <span class="math notranslate nohighlight">\(\boldsymbol{v}_1\)</span> is called <strong>power iteration</strong> (or the <strong>power method</strong>).</p>
</div>
<div class="note docutils">
<p>The entries in the vector <span class="math notranslate nohighlight">\(A^k \boldsymbol{x}_0\)</span> may get very large as <span class="math notranslate nohighlight">\(k\)</span> increases therefore it is helpful to normalize at each step. The simplest way is to divide by <span class="math notranslate nohighlight">\(\| \boldsymbol{x}_k \|_{\infty} = \max \{\boldsymbol{x}_k \}\)</span></p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{x}_{k+1} = \frac{A \boldsymbol{x}_k}{\| A \boldsymbol{x}_k \|_{\infty}}
\]</div>
<p>This is called <strong>normalized power iteration</strong>. Note that <span class="math notranslate nohighlight">\(\| A \boldsymbol{x}_k \|_{\infty}\)</span> gives an approximation of <span class="math notranslate nohighlight">\(\lambda_1\)</span> at each step.</p>
</div>
<div class="example docutils">
<p>Approximate the dominant eigenvalue and eigenvector of the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix}
\end{split}\]</div>
<p>by 4 iterations of the normalized power method. Choose a random starting vector</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{x}_0 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
\end{split}\]</div>
<p>and compute</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
A \boldsymbol{x}_0 &amp;=
\begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix}
\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
=
\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}
&amp;
\boldsymbol{x}_1 = \frac{A \boldsymbol{x}_0}{\| A \boldsymbol{x}_0 \|_{\infty}}
&amp;=
\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}
\\
A \boldsymbol{x}_1 &amp;=
\begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix}
\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}
=
\begin{bmatrix} 2 \\ 2 \\ 1 \end{bmatrix}
&amp;
\boldsymbol{x}_2 = \frac{A \boldsymbol{x}_1}{\| A \boldsymbol{x}_1 \|_{\infty}}
&amp;=
\begin{bmatrix} 1 \\ 1 \\ 0.5 \end{bmatrix}
\\
A \boldsymbol{x}_2 &amp;=
\begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix}
\begin{bmatrix} 1 \\ 1 \\ 0.5 \end{bmatrix}
=
\begin{bmatrix} 2 \\ 2.5 \\ 1.5 \end{bmatrix}
&amp;
\boldsymbol{x}_3 = \frac{A \boldsymbol{x}_2}{\| A \boldsymbol{x}_2 \|_{\infty}}
&amp;=
\begin{bmatrix} 0.8 \\ 1 \\ 0.6 \end{bmatrix}
\\
A \boldsymbol{x}_3 &amp;=
\begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix}
\begin{bmatrix} 0.8 \\ 1 \\ 0.6 \end{bmatrix}
=
\begin{bmatrix} 1.8 \\ 2.4 \\ 1.6 \end{bmatrix}
&amp;
\boldsymbol{x}_4 = \frac{A \boldsymbol{x}_3}{\| A \boldsymbol{x}_3 \|_{\infty}}
&amp;=
\begin{bmatrix} 0.75 \\ 1 \\ 0.67 \end{bmatrix}
\end{align*}
\end{split}\]</div>
<p>Therefore we get approximations</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\lambda_1 \approx 2.4
\hspace{10mm}
\boldsymbol{v}_1 \approx \begin{bmatrix} 0.75 \\ 1 \\ 0.67 \end{bmatrix}
\end{split}\]</div>
<p>The actual dominant eigenvector is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{v}_1 = \begin{bmatrix} 1/\sqrt{2} \\ 1 \\ 1/\sqrt{2} \end{bmatrix}
\end{split}\]</div>
<p>and we verify</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{bmatrix}
\begin{bmatrix} 1/\sqrt{2} \\ 1 \\ 1/\sqrt{2} \end{bmatrix}
=
\begin{bmatrix} \frac{1 + \sqrt{2}}{\sqrt{2}} \\ 1 + \sqrt{2} \\ \frac{1 + \sqrt{2}}{\sqrt{2}} \end{bmatrix}
=
(1 + \sqrt{2}) \begin{bmatrix} \frac{1}{\sqrt{2}} \\ 1 \\ \frac{1}{\sqrt{2}} \end{bmatrix}
\end{split}\]</div>
<p>therefore <span class="math notranslate nohighlight">\(\lambda_1 \approx 2.4142\)</span>.</p>
</div>
</section>
<section id="rayleigh-quotient">
<h2>Rayleigh Quotient<a class="headerlink" href="#rayleigh-quotient" title="Permalink to this heading">#</a></h2>
<div class="definition docutils">
<p>Note that if <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is an eigenvector of a matrix <span class="math notranslate nohighlight">\(A\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span> then</p>
<div class="math notranslate nohighlight">
\[
A \boldsymbol{x} = \lambda \boldsymbol{x}
\hspace{5mm}
\Rightarrow
\hspace{5mm}
\boldsymbol{x}^T A \boldsymbol{x} = \boldsymbol{x}^T (\lambda \boldsymbol{x})
\hspace{5mm}
\Rightarrow
\hspace{5mm}
\lambda = \frac{\boldsymbol{x}^T A \boldsymbol{x}}{ \boldsymbol{x}^T \boldsymbol{x} }
\]</div>
<p>Therefore if <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is an approximate eigenvector of <span class="math notranslate nohighlight">\(A\)</span> then an approximation of the corresponding eigenvalue is given by the <strong>Rayleigh quotient</strong></p>
<div class="math notranslate nohighlight">
\[
\frac{\boldsymbol{x}^T A \boldsymbol{x}}{ \boldsymbol{x}^T \boldsymbol{x} }
\]</div>
<p>In particular, in the power method, the sequence of Rayleigh quotients</p>
<div class="math notranslate nohighlight">
\[
\lambda_k = \frac{\boldsymbol{x}_k^T A \boldsymbol{x}_k}{ \boldsymbol{x}_k^T \boldsymbol{x}_k }
\]</div>
<p>converges to the dominant eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
</div>
</section>
<section id="inverse-iteration">
<h2>Inverse Iteration<a class="headerlink" href="#inverse-iteration" title="Permalink to this heading">#</a></h2>
<div class="definition docutils">
<p>Let <span class="math notranslate nohighlight">\(A\)</span> be an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix with eigenvalues <span class="math notranslate nohighlight">\(\lambda_1,\dots,\lambda_n\)</span> (in increasing order <span class="math notranslate nohighlight">\(\lambda_1 &lt; \lambda_2 &lt; \cdots &lt; \lambda_n\)</span>) with corresponding eigenvectors <span class="math notranslate nohighlight">\(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n\)</span>. Then <span class="math notranslate nohighlight">\(1/\lambda_1,\dots,1/\lambda_n\)</span> are the eigenvalues of <span class="math notranslate nohighlight">\(A^{-1}\)</span> (in decreasing order) with corresponding eigenvectors <span class="math notranslate nohighlight">\(\boldsymbol{v}_1,\dots,\boldsymbol{v}_n\)</span>. <strong>Inverse iteration</strong> is power iteration applied to <span class="math notranslate nohighlight">\(A^{-1}\)</span> to find the dominant eigenvalue <span class="math notranslate nohighlight">\(1/\lambda_n\)</span> of <span class="math notranslate nohighlight">\(A^{-1}\)</span> (equivalently, the smallest eigenvalue <span class="math notranslate nohighlight">\(\lambda_n\)</span> of <span class="math notranslate nohighlight">\(A\)</span>) with eigenvector <span class="math notranslate nohighlight">\(\boldsymbol{v}_n\)</span>. At each step, solve the system and normalize</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y}_{k+1} = A^{-1} \boldsymbol{x}_k \ \ \Rightarrow \ \ A \boldsymbol{y}_{k+1} = \boldsymbol{x}_k \ \ \Rightarrow \ \ \boldsymbol{x}_{k+1} = \frac{\boldsymbol{y}_{k+1}}{\| \boldsymbol{y}_{k+1} \|_{\infty}}
\]</div>
</div>
<div class="example docutils">
<p>Compute 2 steps of inverse iterations for the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 0 \\ 1 &amp; 0 &amp; 3 \end{bmatrix}
\end{split}\]</div>
<p>Since we are going to repeatedly solve systems <span class="math notranslate nohighlight">\(A \boldsymbol{x} = \boldsymbol{b}\)</span>, we should find the LU decomposition and use forward and backward substitution</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \\ -1 &amp; 1 &amp; 0 \\ -1 &amp; 0 &amp; 1 \end{array} \right]
\begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 0 \\ 1 &amp; 0 &amp; 3 \end{bmatrix}
&amp;=
\left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; -1 \\ 0 &amp; -1 &amp; 2 \end{array} \right] \\
\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{array} \right]
\left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; -1 \\ 0 &amp; -1 &amp; 2 \end{array} \right]
&amp;=
\left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; -1 \\ 0 &amp; \phantom{+}0 &amp; 1 \end{array} \right] \\
\end{align*}
\end{split}\]</div>
<p>Therefore</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = LU =
\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; -1 &amp; \phantom{+}1 \end{array} \right]
\left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; -1 \\ 0 &amp; \phantom{+}0 &amp; 1 \end{array} \right]
\end{split}\]</div>
<p>and in fact we see that <span class="math notranslate nohighlight">\(A\)</span> is positive definite and this is the Cholesky decomposition. Choose a random starting vector</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{x}_0 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
\end{split}\]</div>
<p>and compute</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; -1 &amp; \phantom{+}1 \end{array} \right] \boldsymbol{z}_1 &amp;=  \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
&amp; \boldsymbol{z}_1 &amp;= \left[ \begin{array}{r} 1 \\ -1 \\ -2 \end{array} \right] \\
\left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; -1 \\ 0 &amp; \phantom{+}0 &amp; 1 \end{array} \right] \boldsymbol{y}_1 &amp;= \left[ \begin{array}{r} 1 \\ -1 \\ -2 \end{array} \right]
&amp; \boldsymbol{y}_1 &amp;= \left[ \begin{array}{r} 6 \\ -3 \\ -2 \end{array} \right]
&amp; \boldsymbol{x}_1 &amp;= \left[ \begin{array}{c} 1 \\ -1/2 \\ -1/3 \end{array} \right] \\
\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; -1 &amp; \phantom{+}1 \end{array} \right] \boldsymbol{z}_2 &amp;= \left[ \begin{array}{c} 1 \\ -1/2 \\ -1/3 \end{array} \right]
&amp; \boldsymbol{z}_2 &amp;= \left[ \begin{array}{c} 1 \\ -3/2 \\ -17/6 \end{array} \right] \\
\left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; -1 \\ 0 &amp; \phantom{+}0 &amp; 1 \end{array} \right] \boldsymbol{y}_2 &amp;= \left[ \begin{array}{c} 1 \\ -3/2 \\ -17/6 \end{array} \right]
&amp; \boldsymbol{y}_2 &amp;= \left[ \begin{array}{c} 49/6 \\ -13/3 \\ -17/6 \end{array} \right]
&amp; \boldsymbol{x}_2 &amp;= \left[ \begin{array}{c} 1 \\ -26/49 \\ -17/49 \end{array} \right]
\end{align*}
\end{split}\]</div>
<p>Our approximation of the eigenvector of <span class="math notranslate nohighlight">\(A\)</span> corresponding to the smallest eigenvalue is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{v} = \left[ \begin{array}{c} 1 \\ -26/49 \\ -17/49 \end{array} \right]
\approx
\left[ \begin{array}{r} 1.00 \\ -0.53 \\ -0.35 \end{array} \right]
\end{split}\]</div>
<p>with eigenvalue <span class="math notranslate nohighlight">\(\lambda \approx 0.12\)</span> given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 1 &amp; 2 &amp; 0 \\ 1 &amp; 0 &amp; 3 \end{bmatrix}
\left[ \begin{array}{r} 1.00 \\ -0.53 \\ -0.35 \end{array} \right]
=
\left[ \begin{array}{r} 0.12 \\ -0.06 \\ -0.05 \end{array} \right]
=
0.12 \left[ \begin{array}{r} 1.00 \\ -0.50 \\ -0.42 \end{array} \right]
\end{split}\]</div>
<p>The actual eigenvector is approximately</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{v} \approx \left[ \begin{array}{r} 1.000 \\ -0.532 \\ -0.347 \end{array} \right]
\end{split}\]</div>
<p>with eigenvalue</p>
<div class="math notranslate nohighlight">
\[
\lambda \approx 0.12061476
\]</div>
</div>
</section>
<section id="pagerank">
<h2>PageRank<a class="headerlink" href="#pagerank" title="Permalink to this heading">#</a></h2>
<div class="bigidea docutils">
<p>The PageRank vector is the dominant eigenvector of the adjacency matrix of a directed graph and it ranks the importance of each vertex in the graph.</p>
</div>
<div class="definition docutils">
<p>Consider a directed graph <span class="math notranslate nohighlight">\(G\)</span> with <span class="math notranslate nohighlight">\(N\)</span> vertices (see <a class="reference external" href="https://en.wikipedia.org/wiki/Directed_graph">Wikipedia:Directed graph</a>). The <strong>adjacency matrix</strong> is the <span class="math notranslate nohighlight">\(N \times N\)</span> matrix <span class="math notranslate nohighlight">\(A = [a_{i,j}]\)</span> where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
a_{i,j} = \left\{ \begin{array}{cc} 1 &amp; \text{ if there is an edge to vertex $i$ from vertex $j$} \\ 0 &amp; \text{ if not } \end{array} \right.
\end{split}\]</div>
<p>Suppose the vertices of <span class="math notranslate nohighlight">\(G\)</span> represent a collection webpages and the edges represent links from one webpage to another. (We only count one link maximum from one webpage to another and no links from a webpage to itself.) The <strong>stochastic matrix</strong> of <span class="math notranslate nohighlight">\(G\)</span> represents the process of clicking a random link on a webpage and is given by <span class="math notranslate nohighlight">\(P = [p_{i,j}]\)</span> where</p>
<div class="math notranslate nohighlight">
\[
p_{i,j} = \frac{a_{i,j}}{\text{total $\#$ of links from webpage $j$}}
\]</div>
<p>The entry <span class="math notranslate nohighlight">\(p_{i,j}\)</span> is the probability of clicking to webpage <span class="math notranslate nohighlight">\(i\)</span> from webpage <span class="math notranslate nohighlight">\(j\)</span>.</p>
</div>
<div class="example docutils">
<p>Consider the directed graph</p>
<a class="reference internal image-reference" href="../_images/graph01.png"><img alt="../_images/graph01.png" class="align-center" src="../_images/graph01.png" style="width: 300px;" /></a>
<p>Construct the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span> and the stochastic matrix <span class="math notranslate nohighlight">\(P\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 0 &amp; 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 1 &amp; 0 \end{bmatrix}
\hspace{10mm}
P = \begin{bmatrix} 0 &amp; 1/2 &amp; 1/3 &amp; 0 \\ 1 &amp; 0 &amp; 1/3 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1/2 &amp; 1/3 &amp; 0 \end{bmatrix}
\end{split}\]</div>
</div>
<div class="definition docutils">
<p>The <strong>Google matrix</strong> of a directed graph <span class="math notranslate nohighlight">\(G\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\alpha P + (1 - \alpha) \boldsymbol{v} \boldsymbol{e}^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(P\)</span> is the stochastic matrix of <span class="math notranslate nohighlight">\(G\)</span>, <span class="math notranslate nohighlight">\(0 &lt; \alpha &lt; 1\)</span> is the <strong>teleportation parameter</strong>, <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> is the <strong>teleportation distribution vector</strong> and <span class="math notranslate nohighlight">\(\boldsymbol{e}^T = \begin{bmatrix} 1 &amp; \cdots &amp; 1 \end{bmatrix}\)</span> is a vector of 1s. Note <span class="math notranslate nohighlight">\(\boldsymbol{v} \boldsymbol{e}^T\)</span> is the matrix with vector <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> in every column. See <a class="reference external" href="https://www.cs.purdue.edu/homes/dgleich/publications.html">PageRank Beyond the Web</a> and  <a class="reference external" href="https://en.wikipedia.org/wiki/PageRank">Wikipedia:PageRank</a></p>
</div>
<div class="note docutils">
<p>The teleportation vector <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> has entries between 0 and 1 and the entries sum to 1. In other words, it is a stochastic vector. The vector <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span> is usually chosen to be <span class="math notranslate nohighlight">\(\boldsymbol{v} = (1/N)\boldsymbol{e}\)</span> where <span class="math notranslate nohighlight">\(N\)</span> is the number of vertices in the graph. The stochastic matrix <span class="math notranslate nohighlight">\(\boldsymbol{v} \boldsymbol{e}^T\)</span> then represents the process of transitioning to a random webpage with uniform probability. The Google matrix is a stochastic matrix which represents the process: at each step, do either:</p>
<ul class="simple">
<li><p>probability <span class="math notranslate nohighlight">\(\alpha\)</span>: click a random link on the webpage to visit another webapge</p></li>
<li><p>probability <span class="math notranslate nohighlight">\(1 - \alpha\)</span>: teleport to any webpage according to the distribution <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span></p></li>
</ul>
<p>The teleportation parameter <span class="math notranslate nohighlight">\(\alpha\)</span> is usually chosen to be <span class="math notranslate nohighlight">\(\alpha = 0.85\)</span>.</p>
</div>
<div class="theorem docutils">
<p>Let <span class="math notranslate nohighlight">\(G\)</span> be a directed graph and let <span class="math notranslate nohighlight">\(P\)</span> be the stochastic matrix for <span class="math notranslate nohighlight">\(G\)</span>. Choose parameters <span class="math notranslate nohighlight">\(0 &lt; \alpha &lt; 0\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span>. There exists a unique steady state vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> (with entries between 0 and 1 and the entries sum to 1) such that</p>
<div class="math notranslate nohighlight">
\[
\left( \alpha P + (1 - \alpha) \boldsymbol{v} \boldsymbol{e}^T \right) \boldsymbol{x} = \boldsymbol{x}
\]</div>
<p>The vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> is called the <strong>PageRank vector</strong> and the entry <span class="math notranslate nohighlight">\(x_i\)</span> is the PageRank of the webpage at vertex <span class="math notranslate nohighlight">\(i\)</span>. The Google search result lists the webpages in order of their PageRank.</p>
</div>
<div class="note docutils">
<p>A directed graph <span class="math notranslate nohighlight">\(G\)</span> represents a collection of webpages that contain the words in a Google search. The PageRank vector ranks the importance of the webpages for the search. There are usually hundreds of millions webpages in the graph therefore the Google matrix is HUGE! But the founders of Google showed that the power iteration algorithm converges well enough after about 50 iterations to find the webpages with the top PageRank.</p>
</div>
<div class="example docutils">
<p>Find the Google matrix for the directed graph in the example above for <span class="math notranslate nohighlight">\(\alpha = 0.85\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{v} = (1/N) \boldsymbol{e}\)</span>. Compute</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\alpha P + (1 - \alpha) \boldsymbol{v} \boldsymbol{e}^T &amp;=
0.85 \begin{bmatrix} 0 &amp; 1/2 &amp; 1/3 &amp; 0 \\ 1 &amp; 0 &amp; 1/3 &amp; 1 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1/2 &amp; 1/3 &amp; 0 \end{bmatrix}
+
\frac{0.15}{4}
\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 &amp; 1 \end{bmatrix} \\
&amp;=
\begin{bmatrix}
0.0375 &amp; 0.4625 &amp; 0.3208 &amp; 0.0375 \\
0.8875 &amp; 0.0375 &amp; 0.3208 &amp; 0.8875 \\
0.0375 &amp; 0.0375 &amp; 0.0375 &amp; 0.0375 \\
0.0375 &amp; 0.4625 &amp; 0.3208 &amp; 0.0375
\end{bmatrix}
\end{align*}
\end{split}\]</div>
<p>Compute 50 iterations of the power method to approximate the PageRank vector</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{x} \approx \begin{bmatrix} 0.2472 \\ 0.4681 \\ 0.0375 \\ 0.2472 \end{bmatrix}
\end{split}\]</div>
<p>Clearly, vertex 2 is the most important in the graph.</p>
</div>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">#</a></h2>
<p><strong>Exercise 1.</strong> Determine whether the statement is <strong>True</strong> or <strong>False</strong>.</p>
<ul class="simple">
<li><p>The inverse iteration algorithm (without normalization) computes a recursive sequence <span class="math notranslate nohighlight">\( A \boldsymbol{x}_{k+1} = \boldsymbol{x}_k \)</span> where <span class="math notranslate nohighlight">\( \boldsymbol{x}_k \)</span> converges to:</p></li>
<li><p>the largest (in absolute value <span class="math notranslate nohighlight">\( | \lambda | \)</span>) eigenvalue of <span class="math notranslate nohighlight">\( A \)</span></p></li>
<li><p>an eigenvector corresponding to the largest (in absolute value <span class="math notranslate nohighlight">\( | \lambda | \)</span>) eigenvalue of <span class="math notranslate nohighlight">\( A \)</span></p></li>
<li><p>the smallest (in absolute value <span class="math notranslate nohighlight">\( | \lambda | \)</span>) eigenvalue of <span class="math notranslate nohighlight">\( A \)</span></p></li>
<li><p>an eigenvector corresponding to the smallest (in absolute value <span class="math notranslate nohighlight">\( | \lambda | \)</span>) eigenvalue of <span class="math notranslate nohighlight">\( A \)</span></p></li>
<li><p>In the power iteration algorithm, we divide by <span class="math notranslate nohighlight">\( \| A \boldsymbol{x}_k \|_{\infty} \)</span> in each step to:</p></li>
<li><p>make the algorithm run faster</p></li>
<li><p>prevent the entries of the vectors <span class="math notranslate nohighlight">\( \boldsymbol{x}_k \)</span> from becoming too large/small</p></li>
<li><p>produce a more accurate result</p></li>
<li><p>It is necessary to compute all the eigenvectors of the Google matrix to find the PageRank vector of a directed graph.</p></li>
</ul>
<p><strong>Exercise 2.</strong> Let <span class="math notranslate nohighlight">\( A \)</span> be a <span class="math notranslate nohighlight">\( 2 \times 2 \)</span> matrix with eigenvalues <span class="math notranslate nohighlight">\( \lambda_1 = 1 \)</span> and <span class="math notranslate nohighlight">\( \lambda_2 = 1/2 \)</span> and corresponding eigenvectors</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{v}_1 = \begin{bmatrix} 1 \\ 2 \end{bmatrix} \hspace{10mm} \boldsymbol{v}_2 = \begin{bmatrix} -1 \\ 1 \end{bmatrix}
\end{split}\]</div>
<p>If we choose <span class="math notranslate nohighlight">\( \boldsymbol{x}_0 = \begin{bmatrix} 1 \\ 5 \end{bmatrix} \)</span> then the sequence <span class="math notranslate nohighlight">\( \boldsymbol{x}_{k+1} = A \boldsymbol{x}_k \)</span> converges to what?</p>
<p><strong>Exercise 3.</strong> Consider the same directed graph as in the example in the section on PageRank:</p>
<a class="reference internal image-reference" href="../_images/graph01.png"><img alt="../_images/graph01.png" class="align-center" src="../_images/graph01.png" style="width: 300px;" /></a>
<p>As <span class="math notranslate nohighlight">\(\alpha \to 1\)</span>, describe what happens to the PageRank <span class="math notranslate nohighlight">\(x_3\)</span> of vertex 3.</p>
<p><strong>Exercise 4.</strong> Let <span class="math notranslate nohighlight">\(G\)</span> be the complete directed graph with <span class="math notranslate nohighlight">\(N\)</span> vertices. In other words, there is an edge from each vertex to every other vertex in <span class="math notranslate nohighlight">\(G\)</span> (excluding edges from a vertex to itself). Describe the Google matrix and the PageRank vector for the complete directed graph.</p>
<p><strong>Exercise 5.</strong> Find the Google matrix <span class="math notranslate nohighlight">\(\alpha P + (1 - \alpha) \boldsymbol{v} \boldsymbol{e}^T\)</span> for the directed graph</p>
<a class="reference internal image-reference" href="../_images/graph02.png"><img alt="../_images/graph02.png" class="align-center" src="../_images/graph02.png" style="width: 300px;" /></a>
<p>using teleportation parameter <span class="math notranslate nohighlight">\(\alpha=0.5\)</span> and uniform distribution vector <span class="math notranslate nohighlight">\(\boldsymbol{v}\)</span>. Let <span class="math notranslate nohighlight">\(\boldsymbol{x}_0 = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}^T\)</span> and use Python to compute 50 iterations of the power method to approximate the PageRank vector.</p>
<p><strong>Exercise 6.</strong> Find the Google matrix <span class="math notranslate nohighlight">\(\alpha P + (1 - \alpha) \boldsymbol{v} \boldsymbol{e}^T\)</span> for the directed graph in the previous exercise using teleportation parameter <span class="math notranslate nohighlight">\(\alpha=0.8\)</span> and distribution vector <span class="math notranslate nohighlight">\(\boldsymbol{v} =  \begin{bmatrix} 0 &amp; 1/2 &amp; 1/2 &amp; 0 \end{bmatrix}^T\)</span>. Let <span class="math notranslate nohighlight">\(\boldsymbol{x}_0 = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}^T\)</span> and use Python to compute 50 iterations of the power method to approximate the PageRank vector.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./eigenvalues"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="svd.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Singular Value Decomposition</p>
      </div>
    </a>
    <a class="right-next"
       href="../dft/complex.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Complex Vectors</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#power-method">Power Method</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rayleigh-quotient">Rayleigh Quotient</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inverse-iteration">Inverse Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pagerank">PageRank</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By UBC Math
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>